{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import pearsonr, spearmanr, pointbiserialr\n",
    "from sklearn.linear_model import Lasso,ElasticNet,Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, validation_curve, GridSearchCV, train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score, make_scorer\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from category_encoders import TargetEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from pprint import pprint\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from category_encoders.target_encoder import TargetEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from numpy import array  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re2 as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.read_csv('all_df_350.csv', index_col=0)\n",
    "feat_ad=pd.read_csv('adgraph/ad_features.csv',index_col=0)\n",
    "label_ad=pd.read_csv('adgraph/ad_labelled.csv',index_col=0)\n",
    "feat_web=pd.read_csv('webgraph/web_features.csv',index_col=0)\n",
    "label_web=pd.read_csv('webgraph/web_labelled.csv',index_col=0)\n",
    "\n",
    "df_labelled_ad = feat_ad.merge(label_ad[['visit_id', 'name', 'label']], on=['visit_id', 'name'])\n",
    "df_labelled_ad = df_labelled_ad[df_labelled_ad['label'] != \"Error\"]\n",
    "\n",
    "df_labelled_web = feat_web.merge(label_web[['visit_id', 'name', 'label']], on=['visit_id', 'name'])\n",
    "df_labelled_web = df_labelled_web[df_labelled_web['label'] != \"Error\"]\n",
    "\n",
    "df_word_embed = feat_ad.merge(label_ad[['visit_id', 'name', 'label', 'top_level_url']], on=['visit_id', 'name'])\n",
    "df_word_embed = df_word_embed[df_word_embed['label'] != \"Error\"]\n",
    "df_word_embed = df_word_embed[['visit_id', 'name', 'top_level_url', 'label']]\n",
    "\n",
    "df_word_embed['top_level_url'] = df_word_embed['top_level_url'].str.replace(\"http://\", \"\").str.replace(\"https://\", \"\")\n",
    "df_word_embed['top_level_url'] = df_word_embed['top_level_url'].str.replace(\"[\", \"%5B\").str.replace(\"]\", \"%5D\")\n",
    "train, test = train_test_split(df_word_embed, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model load\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 664128/664128 [01:20<00:00, 8291.46it/s]\n",
      "100%|██████████| 166032/166032 [00:20<00:00, 8173.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model load\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 664128/664128 [00:24<00:00, 26969.73it/s]\n",
      "100%|██████████| 166032/166032 [00:05<00:00, 29773.19it/s]\n"
     ]
    }
   ],
   "source": [
    "feat_cont = pd.DataFrame(df_labelled_ad.content_policy_type)\n",
    "# feat_cont.reset_index(inplace=True, drop=True)\n",
    "feat_cont['name'] = df_labelled_ad.content_policy_type\n",
    "feat_cont.drop_duplicates(inplace = True, subset='content_policy_type', keep = \"last\")\n",
    "feat_cont.set_index('content_policy_type',inplace=True)\n",
    "feat_cont.to_dict()\n",
    "\n",
    "ad_col = df_labelled_ad.columns\n",
    "web_col = df_labelled_web.columns\n",
    "col_list = []\n",
    "\n",
    "for i in range(len(ad_col)):\n",
    "    for j in range(len(web_col)):\n",
    "        if ad_col[i] == web_col[j]:\n",
    "            col_list.append(ad_col[i])\n",
    "# col_list = col_list[2:-1]\n",
    "df_outer_web = df_labelled_web.copy().drop(col_list, axis=1)\n",
    "integ_df = pd.concat([df_labelled_ad.drop('label', axis=1), df_outer_web, df_labelled_ad['label']], axis=1)    ### AdGraph, WebGraph Features\n",
    "\n",
    "_istarget=True\n",
    "target_word_ft, target_word_ft_test = wordFeature(list(train['name']), list(test['name']), _istarget)\n",
    "_istarget=False\n",
    "source_word_ft, source_word_ft_test = wordFeature(list(train['top_level_url']), list(test['top_level_url']),_istarget)\n",
    "\n",
    "fqdn_col = []\n",
    "for i in range(0, 30):\n",
    "    fqdn_col.append(\"fqdn_\" + str(i))\n",
    "\n",
    "source_word_ft.columns = fqdn_col\n",
    "source_word_ft_test.columns = fqdn_col\n",
    "    \n",
    "req_col = []\n",
    "for i in range(0, 200):\n",
    "    req_col.append(\"req_\" + str(i))\n",
    "    \n",
    "target_word_ft.columns = req_col\n",
    "target_word_ft_test.columns = req_col\n",
    "\n",
    "train_url230_sg = pd.concat([source_word_ft, target_word_ft], axis=1)\n",
    "test_url230_sg = pd.concat([source_word_ft_test, target_word_ft_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_883 = pd.read_csv('all_df_883.csv', index_col=0)\n",
    "\n",
    "new_features = all_df_883[all_df_883.columns[303:]].copy()\n",
    "new_features.drop(['contentname', 'label'], axis=1, inplace=True)\n",
    "new_features.fillna(0, inplace=True)\n",
    "\n",
    "ngram_train, ngram_test = train_test_split(new_features, test_size=0.2, random_state=42)\n",
    "ngram_train.reset_index(drop=True, inplace=True)\n",
    "ngram_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "integ_train, integ_test = train_test_split(integ_df, test_size=0.2, random_state=42)\n",
    "integ_train.reset_index(drop=True, inplace=True)\n",
    "integ_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train_df = pd.concat([integ_train[integ_train.columns[:-1]], train_url230_sg, ngram_train, integ_train['label']], axis=1)\n",
    "test_df = pd.concat([integ_test[integ_test.columns[:-1]], test_url230_sg, ngram_test, integ_test['label']], axis=1)\n",
    "\n",
    "sum_resolve = pd.read_csv('sum_resolve.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_resolve, test_resolve = train_test_split(sum_resolve, test_size=0.2, random_state=42)\n",
    "train_resolve.reset_index(drop=True, inplace=True)\n",
    "test_resolve.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('all_df_883_train.csv', index_col=0)\n",
    "test_df = pd.read_csv('all_df_883_test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[train_resolve.index, train_resolve.columns] = train_resolve\n",
    "test_df.loc[test_resolve.index, test_resolve.columns] = test_resolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_indent = train_df['avg_ident'] ** -1\n",
    "# for i in range(len(avg_indent)):\n",
    "#     if avg_indent[i] == np.inf:\n",
    "#         avg_indent[i] = 0\n",
    "# train_df['avg_ident'] = avg_indent\n",
    "\n",
    "# avg_indent = test_df['avg_ident'] ** -1\n",
    "# for i in range(len(avg_indent)):\n",
    "#     if avg_indent[i] == np.inf:\n",
    "#         avg_indent[i] = 0\n",
    "# test_df['avg_ident'] = avg_indent\n",
    "\n",
    "# train_df.to_csv('all_df_883_train.csv')\n",
    "# test_df.to_csv('all_df_883_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Te = TargetEncoder()\n",
    "Te.fit(train_df.content_policy_type, train_df.label)\n",
    "train_df.content_policy_type = Te.transform(train_df.content_policy_type)\n",
    "test_df.content_policy_type = Te.transform(test_df.content_policy_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "filtered_feat = []\n",
    "for i in train_df.columns[2:-1]:\n",
    "    correlation, p_value = pointbiserialr(train_df.label, train_df[i])\n",
    "    if ((p_value > 0.1)):\n",
    "        print(i)\n",
    "        print('Correlation coefficient:', correlation)\n",
    "        print('p-value:', p_value)\n",
    "        cnt += 1\n",
    "        filtered_feat.append(i)\n",
    "train_df.drop(filtered_feat, inplace=True, axis=1)\n",
    "print('Removed : ', cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(['visit_id', 'name', 'label'], axis=1)\n",
    "y = train_df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = train_df[['avg_charperline',\n",
    "'ast_breadth',\n",
    "'ast_depth',\n",
    "'brackettodot',\n",
    "'avg_ident']]\n",
    "\n",
    "temp_label = train_df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_importances\n",
      "result\n",
      "           feature  importance\n",
      "0  avg_charperline    0.090857\n",
      "1      ast_breadth    0.086631\n",
      "2        ast_depth    0.052890\n",
      "3     brackettodot    0.064933\n",
      "4        avg_ident    0.075309\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(temp_df, temp_label, shuffle=True, test_size=0.2, random_state=42) # train / vaild\n",
    "# Train a RandomForest model\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "# Convert the importances into a DataFrame\n",
    "feature_importances = pd.DataFrame({\"feature\": X_train.columns.to_list(), \"importance\": importances})\n",
    "print(\"feature_importances\")\n",
    "# Get permutation importances\n",
    "result = permutation_importance(clf, X_valid, y_valid, n_repeats=5, random_state=42, n_jobs=-1)\n",
    "print(\"result\")\n",
    "# Convert the importances into a DataFrame\n",
    "perm_importances = pd.DataFrame({\"feature\": X_train.columns.to_list(), \"importance\": result.importances_mean})\n",
    "\n",
    "# Print permutation importances\n",
    "print(perm_importances)\n",
    "\n",
    "feature_importances.columns = ['feature', 'RFI_importance']\n",
    "perm_importances.columns = ['feature', 'PI_importance']\n",
    "concat = feature_importances.copy()\n",
    "concat = pd.concat([concat, perm_importances.PI_importance], axis=1)\n",
    "concat['RFI_PI_MEAN'] = (concat.RFI_importance + concat.PI_importance) / 2\n",
    "concat\n",
    "concat.to_csv('RFI_PI_js.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, shuffle=True, test_size=0.2, random_state=42) # train / vaild\n",
    "# Train a RandomForest model\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "# Convert the importances into a DataFrame\n",
    "feature_importances = pd.DataFrame({\"feature\": X_train.columns.to_list(), \"importance\": importances})\n",
    "print(\"feature_importances\")\n",
    "# Get permutation importances\n",
    "result = permutation_importance(clf, X_valid, y_valid, n_repeats=5, random_state=42, n_jobs=-1)\n",
    "print(\"result\")\n",
    "# Convert the importances into a DataFrame\n",
    "perm_importances = pd.DataFrame({\"feature\": X_train.columns.to_list(), \"importance\": result.importances_mean})\n",
    "\n",
    "# Print permutation importances\n",
    "print(perm_importances)\n",
    "\n",
    "feature_importances.columns = ['feature', 'RFI_importance']\n",
    "perm_importances.columns = ['feature', 'PI_importance']\n",
    "concat = feature_importances.copy()\n",
    "concat = pd.concat([concat, perm_importances.PI_importance], axis=1)\n",
    "concat['RFI_PI_MEAN'] = (concat.RFI_importance + concat.PI_importance) / 2\n",
    "concat\n",
    "concat.to_csv('RFI_PI_1006.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphFeatures=[  # 52\n",
    "    \"num_nodes\",\n",
    "    \"num_edges\",\n",
    "    \"nodes_div_by_edges\",\n",
    "    \"edges_div_by_nodes\",\n",
    "    \"in_degree\",\n",
    "    \"out_degree\",\n",
    "    \"in_out_degree\",\n",
    "    \"average_degree_connectivity\",\n",
    "    \"is_ancestor_script\",\n",
    "    \"ascendant_has_ad_keyword\",\n",
    "    \"descendant_of_eval_or_function\",\n",
    "    \"ascendant_script_has_eval_or_function\",\n",
    "    \"ascendant_script_has_fp_keyword\",\n",
    "    \"ascendant_script_length\",\n",
    "    \"ancestors\",\n",
    "    \"descendants\",\n",
    "    \"closeness_centrality\",\n",
    "    \"eccentricity\",\n",
    "    \"num_script_predecessors\",\n",
    "    \"num_script_successors\",\n",
    "    \"num_requests_received\",\n",
    "    \"num_redirects_sent\",\n",
    "    \"num_redirects_rec\",\n",
    "    \"max_depth_redirect\",\n",
    "    \"indirect_in_degree\",\n",
    "    \"indirect_out_degree\",\n",
    "    \"indirect_ancestors\",\n",
    "    \"indirect_descendants\",\n",
    "    \"indirect_closeness_centrality\",\n",
    "    \"indirect_average_degree_connectivity\",\n",
    "    \"indirect_eccentricity\",\n",
    "    \"indirect_mean_in_weights\",\n",
    "    \"indirect_min_in_weights\",\n",
    "    \"indirect_max_in_weights\",\n",
    "    \"indirect_mean_out_weights\",\n",
    "    \"indirect_min_out_weights\",\n",
    "    \"indirect_max_out_weights\",\n",
    "    \"num_set_get_src\",\n",
    "    \"num_set_mod_src\",\n",
    "    \"num_set_url_src\",\n",
    "    \"num_get_url_src\",\n",
    "    \"num_set_get_dst\",\n",
    "    \"num_set_mod_dst\",\n",
    "    \"num_set_url_dst\",\n",
    "    \"num_get_url_dst\",\n",
    "    \"indirect_all_in_degree\",\n",
    "    \"indirect_all_out_degree\",\n",
    "    \"indirect_all_ancestors\",\n",
    "    \"indirect_all_descendants\",\n",
    "    \"indirect_all_closeness_centrality\",\n",
    "    \"indirect_all_average_degree_connectivity\",\n",
    "    \"indirect_all_eccentricity\"\n",
    "]\n",
    "\n",
    "pi_is_zero = perm_importances.loc[(perm_importances.PI_importance <= 0)].feature.tolist()\n",
    "X_wo_graph = X.drop(graphFeatures + pi_is_zero, axis = 1)\n",
    "\n",
    "X_wo_graph = X_wo_graph[X_wo_graph.columns[:260]]\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_wo_graph, y, shuffle=True, test_size=0.2, random_state=42) # train / vaild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf = RandomForestClassifier(random_state=42, n_estimators=100, n_jobs=-1)\n",
    "# rf.fit(X_train, y_train)\n",
    "# print(\"rf.fit(X_train, y_train)\")\n",
    "# result = permutation_importance(rf, X_train, y_train)\n",
    "# perm_importance = result.importances_mean.argsort()\n",
    "\n",
    "# rfi_pi_mean = concat['RFI_PI_MEAN'].copy()\n",
    "\n",
    "min_features_to_select = 10\n",
    "step = 1\n",
    "selector = RFECV(clf, step=step, cv=5, min_features_to_select=min_features_to_select, scoring='accuracy', n_jobs=-1)\n",
    "print(\"RFECV\")\n",
    "\n",
    "selector = selector.fit(X_wo_graph, y)\n",
    "print(\"selector\")\n",
    "\n",
    "rfecv_support = selector.support_\n",
    "rfecv_ranking = selector.ranking_\n",
    "print(\"RFECV support:\", rfecv_support)\n",
    "print(\"RFECV ranking:\", rfecv_ranking)\n",
    "selected_perm_importance = perm_importances[rfecv_support]\n",
    "print(\"Permutation importance for the selected features by RFECV:\", selected_perm_importance)\n",
    "\n",
    "mask = selector.get_support()\n",
    "features = np.array(X_wo_graph.columns.to_list())\n",
    "best_features = features[mask]\n",
    "\n",
    "print(\"All features: \", X_wo_graph.columns.to_list())\n",
    "print(features)\n",
    "\n",
    "print(\"Selected best: \", best_features.shape[0])\n",
    "print(features[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_pearsonr = []\n",
    "for i in range(len(X_wo_graph.columns)):\n",
    "    for j in range(i, len(X_wo_graph.columns)):\n",
    "        if pearsonr(X_wo_graph[X_wo_graph.columns[i]], X_wo_graph[X_wo_graph.columns[j]])[1] >= 0.05:\n",
    "            print(f\"{X_wo_graph.columns[i]}, {X_wo_graph.columns[j]} : {pearsonr(X_wo_graph[X_wo_graph.columns[i]], X_wo_graph[X_wo_graph.columns[j]])[1]}\")\n",
    "            if (X_wo_graph.columns[i].find('req_') != -1) | (X_wo_graph.columns[i].find('fqdn_') != -1):\n",
    "                res_pearsonr.append(X_wo_graph.columns[i])\n",
    "            else:\n",
    "                res_pearsonr.append(X_wo_graph.columns[j])\n",
    "                \n",
    "res_spearman = []\n",
    "\n",
    "for i in range(len(X_wo_graph.columns)):\n",
    "    for j in range(i, len(X_wo_graph.columns)):\n",
    "        if spearmanr(X_wo_graph[X_wo_graph.columns[i]], X_wo_graph[X_wo_graph.columns[j]]).pvalue >= 0.05:\n",
    "            print(f\"{X_wo_graph.columns[i]}, {X_wo_graph.columns[j]} : {spearmanr(X_wo_graph[X_wo_graph.columns[i]], X_wo_graph[X_wo_graph.columns[j]]).pvalue}\")\n",
    "            if (X_wo_graph.columns[i].find('req_') != -1) | (X_wo_graph.columns[i].find('fqdn_') != -1):\n",
    "                res_spearman.append(X_wo_graph.columns[i])\n",
    "            else:\n",
    "                res_spearman.append(X_wo_graph.columns[j])\n",
    "\n",
    "corr_list =pd.Series(res_pearsonr + res_spearman).unique()\n",
    "corr_list\n",
    "\n",
    "print(f\"Final Selected Features : \\n {[i for i in list(features[mask]) if i not in corr_list]} \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webtrack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
